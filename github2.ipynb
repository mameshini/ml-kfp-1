{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end experiment: Github Issue Summarization\n",
    "\n",
    "Currently, this notebook must be run from the Kubeflow JupyterHub installation, as described in the codelab.\n",
    "\n",
    "In this notebook, we will show how to:\n",
    "\n",
    "* Interactively define a KubeFlow Pipeline using the Pipelines Python SDK\n",
    "* Submit and run the pipeline\n",
    "* Add a step in the pipeline\n",
    "\n",
    "This example pipeline trains a [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor/) model on Github issue data, learning to predict issue titles from issue bodies. It then exports the trained model and deploys the exported model to [Tensorflow Serving](https://github.com/tensorflow/serving). \n",
    "The final step in the pipeline launches a web app which interacts with the TF-Serving instance in order to get model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviroinment Setup\n",
    "\n",
    "Before any experiment can be conducted. We need to setup and initialize an environment: ensure all Python modules has been setup and configured, as well as python modules\n",
    "\n",
    "Setting up python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re, kfp, kfp.notebook, extensions, sys\n",
    "if sys.version_info.major != 3:\n",
    "    raise ValueError('We only support Python 3; recommended Python 3.6')\n",
    "\n",
    "# !pip3 install --upgrade 'pip' > /dev/null\n",
    "# !pip3 install --upgrade 'https://storage.googleapis.com/ml-pipeline/release/0.1.18/kfp.tar.gz' > /dev/null\n",
    "# !pip3 install --upgrade './extensions' > /dev/null\n",
    "\n",
    "%load_ext extensions\n",
    "\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import pandas as pd\n",
    "from ipython_secrets import get_secret\n",
    "from kfp.compiler import Compiler\n",
    "from os import environ\n",
    "\n",
    "import extensions.kaniko as kaniko\n",
    "import extensions.pv as pv\n",
    "import extensions.kubernetes as k8s\n",
    "import extensions.kaniko.aws as aws\n",
    "import extensions.seldon as seldon\n",
    "import extensions.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kubenretes secrets to deliver configurqtion management data. However you can define your own. If secret `key` has not been found in the secret, user will be asked to enter the secret value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_ENDPOINT = get_secret('S3_ENDPOINT')\n",
    "DOCKER_REGISTRY = get_secret('DOCKER_REGISTRY')\n",
    "DOCKER_REGISTRY_SECRET = get_secret('DOCKER_REGISTRY_PULL_SECRET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize global namespace variables. It is a good practice to place all global namespace variables in one cell. So, the notebook could be configured all-at-once. \n",
    "\n",
    "To enhance readability we would advice to capitalize such variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = 'john.doe'\n",
    "NAME = re.sub(r'\\W+', '-', USER).lower()\n",
    "rubbish = 'latest'\n",
    "\n",
    "BUILD_CONTEXT = f\"buildcontext-{rubbish}\"\n",
    "\n",
    "EXPERIMENT_NAME = f'Github Issues {USER}'\n",
    "# DOCKER_TAG = 'latest'\n",
    "DOCKER_TAG = rubbish\n",
    "TRAINING_IMAGE = f\"{DOCKER_REGISTRY}/library/training:{DOCKER_TAG}\"\n",
    "SERVING_IMAGE = f\"{DOCKER_REGISTRY}/library/seldon:{DOCKER_TAG}\"\n",
    "\n",
    "# we want to create a unique bucket for each user\n",
    "S3_BUCKET = f\"{NAME}-bucket\" \n",
    "PVC_NAME = f\"{S3_BUCKET}-volume\"\n",
    "MOUNT_PATH = f\"/mnt/s3\"\n",
    "# TRAINING_DIR = f\"{MOUNT_PATH}/training-{rubbish}\"\n",
    "\n",
    "DATASET_FILE = f\"{MOUNT_PATH}/training-{rubbish}/dataset.csv\"\n",
    "MODEL_FILE = f\"{MOUNT_PATH}/training-{rubbish}/training1.h5\"\n",
    "TITLE_PP_FILE = f\"{MOUNT_PATH}/training-{rubbish}/title_preprocessor.dpkl\"\n",
    "BODY_PP_FILE = f\"{MOUNT_PATH}/training-{rubbish}/body_preprocessor.dpkl\"\n",
    "TRAIN_DF_FILE = f\"{MOUNT_PATH}/training-{rubbish}/traindf.csv\"\n",
    "TEST_DF_FILE =  f\"{MOUNT_PATH}/training-{rubbish}/testdf.csv\"\n",
    "TRAIN_TITLE_VECS = f\"{MOUNT_PATH}/training-{rubbish}/train_title_vecs.npy\"\n",
    "TRAIN_BODY_VECS = f\"{MOUNT_PATH}/training-{rubbish}/train_body_vecs.npy\"\n",
    "\n",
    "# github issues small: 2Mi data set (best for dev/test)\n",
    "SAMPLE_DATA_SET = 'https://s3.us-east-2.amazonaws.com/asi-kubeflow-models/gh-issues/data-sample.csv'\n",
    "# data set for 3Gi. (best for training)\n",
    "FULL_DATA_SET = 'https://s3.us-east-2.amazonaws.com/asi-kubeflow-models/gh-issues/data-full.csv'\n",
    "\n",
    "NAMESPACE = k8s.current_namespace()\n",
    "\n",
    "s3 = boto3.session.Session().client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=get_secret('S3_ACCESS_KEY'),\n",
    "    aws_secret_access_key=get_secret('S3_SECRET_KEY'),\n",
    "    endpoint_url=S3_ENDPOINT\n",
    ")\n",
    "\n",
    "\n",
    "client = kfp.Client()\n",
    "\n",
    "try:\n",
    "    exp = client.get_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "except:\n",
    "    exp = client.create_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bucket\n",
    "\n",
    "\n",
    "Here we will generate a new bucket and create a `pvc` that will represent it as a file system inside of the wofkflow pod. To do this we need to define few variables\n",
    "- `S3_BUCKET` - name of the bucket to create. By defaults we mutate name of the user\n",
    "- `NAMESPACE` - points to the current namespace.\n",
    "- `PVC_NAME` - derived form `S3_BUCKET`. This is a kubernetes `pvc` name. This name will be used by pipeline container (`ContainerOp` objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    s3.create_bucket(Bucket=S3_BUCKET, ACL='private')\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] != 'BucketAlreadyOwnedByYou':\n",
    "        raise e\n",
    "\n",
    "%templatefile templates/bucket-pvc.yaml -o bucket-volume.yaml\n",
    "!kubectl apply -f bucket-volume.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dockerfile templates\n",
    "\n",
    "Docker images can be rendered via `%%template` or `%templatefile` magics (source code [here](extensions/magics/templates.py)). It can intelligently use mustache `{{placeholder}}` templating syntax. Content will be replaced by the user namespace defined variable or system environment variable\n",
    "\n",
    "You can use flags with the magic function:\n",
    "* `-v` - to see content of rendered file. \n",
    "* `-h` - for more options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template Dockerfile.keras\n",
    "ARG buildfrom=python:3.6\n",
    "FROM ${buildfrom}\n",
    "ENV PATH \"/src:${PATH}\"\n",
    "\n",
    "WORKDIR /tmp\n",
    "\n",
    "# numpy must be the same version between training and serving\n",
    "# it is specified via requirements.txt\n",
    "RUN pip3 uninstall --yes numpy \\\n",
    "    && pip3 install --upgrade --no-cache-dir pip\n",
    "\n",
    "COPY src/keras-requirements.txt /tmp/requirements.txt\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    python-pandas git \\\n",
    "    && git clone https://github.com/google/seq2seq.git \\\n",
    "    && pip3 install -U -r /tmp/requirements.txt \\\n",
    "    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* \\\n",
    "       /usr/share/man /usr/share/doc /usr/share/doc-base\n",
    "\n",
    "COPY src /src\n",
    "WORKDIR /src\n",
    "\n",
    "ENTRYPOINT /usr/local/bin/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipeline to build images\n",
    "\n",
    "Define build pipeline. Yes, we arguably using KFP to build images  that will be de-facto used by final pipeline.\n",
    "\n",
    "We use [Kaniko](https://github.com/GoogleContainerTools/kaniko) and Kubernetes to handle build operations. Build status can be tracked via KFP pipeline dashboard\n",
    "\n",
    "In fact build image job can be even combined with primary pipeline as physically it will be different Kubernetes pods. However for sake of general purpose efficiency we schedule build process via separate pipeline step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='Pipeline images',\n",
    "  description='Build images that will be used by the pipeline'\n",
    ")\n",
    "def build_images(image, build_context, dockerfile):\n",
    "    dsl.ContainerOp(\n",
    "        name='build-image',\n",
    "        image='gcr.io/kaniko-project/executor:latest',\n",
    "        arguments=['--cache',\n",
    "                   '--destination', image,\n",
    "                   '--dockerfile', dockerfile,\n",
    "                   '--context', build_context]\n",
    "    ).apply(\n",
    "        # docker registry credentials \n",
    "        kaniko.use_pull_secret_projection(secret_name=DOCKER_REGISTRY_SECRET)\n",
    "    ).apply(\n",
    "        # s3 bucket volume clame has been injected here        \n",
    "        pv.use_pvc(name=PVC_NAME, mount_to=MOUNT_PATH)\n",
    "    )\n",
    "        \n",
    "Compiler().compile(build_images, 'kaniko.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiler transforms Python DSL into an [Argo Workflow](https://argoproj.github.io/docs/argo/readme.html). And stores generated artifacts in `kaniko.tar.gz`. So it could be executed multiple times. Perhaps with different parameters\n",
    "\n",
    "Next section will upload all files to s3, to share access with the pipeline. Files that should be ignored can be customized in [kanikoignore.txt](./kanikoignore.txt). To understand upload scenario you can review and modify: [aws.py](./extensions/kaniko/aws.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_keys = s3.list_objects(\n",
    "    Bucket=S3_BUCKET,\n",
    "    Prefix=BUILD_CONTEXT\n",
    ").get('Contents')\n",
    "\n",
    "aws.upload_to_s3(\n",
    "    destination=f\"s3://{S3_BUCKET}/{BUILD_CONTEXT}\",\n",
    "    ignorefile='kanikoignore.txt',\n",
    "    workspace='.',\n",
    "    s3_client=s3\n",
    ")\n",
    "\n",
    "run = client.run_pipeline(exp.id, 'Build docker images', 'kaniko.tar.gz', \n",
    "                          params={\n",
    "                              'image': TRAINING_IMAGE,\n",
    "                              'build-context': f\"{MOUNT_PATH}/{BUILD_CONTEXT}\",\n",
    "                              'dockerfile': 'Dockerfile.keras',\n",
    "                          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build process can be long a long term. Because often images that has been used for data science tasks are huge. In this case you might want to adjust `timeout` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# block until job completion\n",
    "print(f\"Waiting for run: {run.id}...\")\n",
    "result = client.wait_for_run_completion(run.id, timeout=720).run.status\n",
    "print(f\"Finished with: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In this chapter we will define a pipeline that will do two important steps. It will download a data set in CSV file format (we call this operation **data import**) and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_op(name, arguments=[]):\n",
    "    \"\"\" A template function to encapsulate similar container ops\n",
    "    \"\"\"\n",
    "    return dsl.ContainerOp(\n",
    "        name=name,\n",
    "        image=TRAINING_IMAGE,\n",
    "        command=['/usr/local/bin/python'],\n",
    "        arguments=arguments\n",
    "    ).apply(\n",
    "        pv.use_pvc(name=PVC_NAME, mount_to=MOUNT_PATH)\n",
    "    )\n",
    "    \n",
    "\n",
    "@dsl.pipeline(\n",
    "  name='Data preparation',\n",
    "  description=\"\"\"Extract validate transform and load data into object storage. \n",
    "  So it could be accessible by the actual training\n",
    "  \"\"\"\n",
    ")\n",
    "def prepare_data(\n",
    "    import_from: dsl.PipelineParam, \n",
    "    dataset_file: dsl.PipelineParam, \n",
    "    train_df_file: dsl.PipelineParam,\n",
    "    test_df_file: dsl.PipelineParam,\n",
    "    title_pp_file: dsl.PipelineParam,\n",
    "    body_pp_file: dsl.PipelineParam,\n",
    "    train_title_vecs: dsl.PipelineParam,\n",
    "    train_body_vecs: dsl.PipelineParam,\n",
    "    model_file: dsl.PipelineParam,\n",
    "    sample_size: dsl.PipelineParam=dsl.PipelineParam(name='sample-size', value='200'),\n",
    "    learning_rate: dsl.PipelineParam=dsl.PipelineParam(name='learning-rate', value=0.001),\n",
    "):  \n",
    "    import_data = dsl.ContainerOp(\n",
    "        name='import-data',\n",
    "        image='appropriate/curl',\n",
    "        arguments=['-#Lv', '--create-dirs', '-o', dataset_file, import_from]\n",
    "    ).apply(\n",
    "        pv.use_pvc(name=PVC_NAME, mount_to=MOUNT_PATH)\n",
    "    )        \n",
    "\n",
    "    # Generates the training and test set. Only processes \"sample-size\" rows.\n",
    "    process_data = training_op(\n",
    "        name='process-data',\n",
    "        arguments=[\n",
    "            'process_data.py', \n",
    "            '--input_csv', dataset_file,\n",
    "            '--sample_size', sample_size,\n",
    "            '--output_traindf_csv', train_df_file, \n",
    "            '--output_testdf_csv', test_df_file,\n",
    "        ]\n",
    "    )\n",
    "    process_data.after(import_data)\n",
    "    \n",
    "    # Preprocess for deep learning\n",
    "    preproc_for_ml = training_op(\n",
    "        name = 'preproc-for-ml',\n",
    "        arguments=[\n",
    "            'preproc.py',\n",
    "            '--input_traindf_csv', train_df_file,\n",
    "            '--output_title_preprocessor_dpkl', title_pp_file,\n",
    "            '--output_body_preprocessor_dpkl', body_pp_file,\n",
    "            '--output_train_title_vecs_npy', train_title_vecs,\n",
    "            '--output_train_body_vecs_npy', train_body_vecs,\n",
    "        ]\n",
    "    )\n",
    "    preproc_for_ml.after(process_data)\n",
    "    \n",
    "    # Training\n",
    "    training = training_op(\n",
    "        name = 'training',\n",
    "        arguments=[\n",
    "            'train.py',\n",
    "            '--input_title_preprocessor_dpkl', title_pp_file,\n",
    "            '--input_body_preprocessor_dpkl', body_pp_file,\n",
    "            '--input_train_title_vecs_npy', train_title_vecs,\n",
    "            '--input_train_body_vecs_npy', train_body_vecs,\n",
    "            '--script_name_base', '/tmp/seq2seq',\n",
    "            '--output_model_h5', model_file,\n",
    "            '--learning_rate', learning_rate,\n",
    "           '--tempfile', \"True\",\n",
    "        ]\n",
    "    )\n",
    "    training.after(preproc_for_ml)\n",
    "        \n",
    "\n",
    "Compiler().compile(prepare_data, 'preproc.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below will run a pipeline and inject some pipeline parameters. Here we provide two versions of data sets\n",
    "* `SAMPLE_DATA_SET` - Data set that has just over 2 megabytes. Not enough for sufficient training. However ideal for development, because of faster feedback.\n",
    "* `FULL_DATA_SET` - Precreated data set with all github issues. 3 gigabytes. Good enough for sufficient model\n",
    "\n",
    "Depending on your needs you can choose one or another data set and pass it as a pipeline parameter `data-set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.run_pipeline(exp.id, 'Prepare data', 'preproc.tar.gz',\n",
    "                          params={\n",
    "                              'import-from': SAMPLE_DATA_SET,\n",
    "                              'dataset-file': DATASET_FILE,\n",
    "                              'title-pp-file': TITLE_PP_FILE,\n",
    "                              'body-pp-file': BODY_PP_FILE,\n",
    "                              'train-df-file': TRAIN_DF_FILE,\n",
    "                              'test-df-file': TEST_DF_FILE,\n",
    "                              'train-title-vecs': TRAIN_TITLE_VECS,\n",
    "                              'train-body-vecs': TRAIN_BODY_VECS,\n",
    "                              'model-file': MODEL_FILE,\n",
    "                              'learning-rate': 0.001,\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# block until job completion\n",
    "print(f\"Waiting for run: {run.id}...\")\n",
    "result = client.wait_for_run_completion(run.id, timeout=720).run.status\n",
    "print(f\"Finished with: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving with Seldon\n",
    "Prepping a container for serving. \n",
    "\n",
    "Here we define all variables that will be needed for our dockerfile tempalte. \n",
    "\n",
    "- `MODEL_WRAPPER`: is a name of a python class that adapts keras model for serving\n",
    "- `MODEL_NAME`: used in seldon deployment\n",
    "- `MODEL_VERSION`: one model can be served multiple times with different versions simulteniously\n",
    "- `SELDON_DEPLOYMENT`: name of kubernetes resource\n",
    "- `SELDON_OAUTH_KEY`: part of shared secret between `SeldonDeployment` and a client application\n",
    "- `SELDON_OAUTH_SECRET`: part of shared secret between `SeldonDeployment` and a client application\n",
    "- `REPLICAS`: number of replicas for `SeldonDeployment` pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_WRAPPER = 'IssueSummarizationModel'\n",
    "MODEL_NAME = re.sub(r'\\W+', '-', MODEL_WRAPPER).lower()\n",
    "MODEL_VERSION = f\"v{rubbish}\"\n",
    "SELDON_DEPLOYMENT = f\"{MODEL_NAME}-{MODEL_VERSION}\"\n",
    "# here we hash a information about model, so it would be predictable\n",
    "SELDON_OAUTH_KEY = utils.sha1(MODEL_NAME, MODEL_VERSION, NAMESPACE)\n",
    "# for secure secret we will use hash of user defined shared secret salted with OAUTH_KEY\n",
    "SELDON_OAUTH_SECRET = utils.sha1(SELDON_OAUTH_KEY, get_secret('USER_PASSWORD_FOR_MODEL'))\n",
    "SELDON_APISERVER_ADDR=f\"seldon-seldon-apiserver.{NAMESPACE}:8080\"\n",
    "\n",
    "SELDON_DEPLOYMENT_REPLICAS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Serving container\n",
    "\n",
    "`SeldonDeployment` needs a docker image that contains a model wrapper written in (but not limited) Python\n",
    "\n",
    "This step will build a container and serve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template Dockerfile.seldon\n",
    "FROM seldonio/seldon-core-s2i-python3\n",
    "\n",
    "ENV SERVICE_TYPE \"MODEL\"\n",
    "ENV PERSISTENCE \"0\"\n",
    "\n",
    "EXPOSE 5000\n",
    "\n",
    "COPY src/serving.py /microservice/{{MODEL_WRAPPER}}.py\n",
    "COPY src/seq2seq_utils.py /microservice\n",
    "COPY src/requirements.txt /tmp/requirements.txt\n",
    "# numpy must be the same version between training and serving\n",
    "# it has been specified via requirements.txt\n",
    "RUN pip3 uninstall --yes numpy \\\n",
    " && pip3 install --upgrade --no-cache-dir pip \\\n",
    " && pip3 install --upgrade --no-cache-dir -r /tmp/requirements.txt\n",
    "\n",
    "ENTRYPOINT [\"python\",\"-u\",\"microservice.py\"]\n",
    "CMD [\"{{MODEL_WRAPPER}}\", \"REST\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to serve trained model we build an image with our serving microservice. To achieve this we reuse our kaniko pipeline defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws.upload_to_s3(\n",
    "    destination=f\"s3://{S3_BUCKET}/{BUILD_CONTEXT}\",\n",
    "    ignorefile='kanikoignore.txt',\n",
    "    workspace='.',\n",
    "    s3_client=s3,\n",
    ")\n",
    "\n",
    "run = client.run_pipeline(exp.id, 'Build a serving image', 'kaniko.tar.gz', \n",
    "                          params={\n",
    "                              'image': SERVING_IMAGE,\n",
    "                              'build-context': f\"{MOUNT_PATH}/{BUILD_CONTEXT}\",\n",
    "                              'dockerfile': 'Dockerfile.seldon',\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# block until job completion\n",
    "print(f\"Waiting for run: {run.id}...\")\n",
    "result = client.wait_for_run_completion(run.id, timeout=720).run.status\n",
    "print(f\"Finished with: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we render our `SeldonDeployment` template and deploy it with `kubectl`, similar as we have done before with `pvc` definition. Here we define reference to the model that will be used for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%templatefile templates/seldon.yaml -o seldon.yaml\n",
    "!kubectl apply -f seldon.yaml --wait\n",
    "!kubectl get -f seldon.yaml -o jsonpath='{.status.state}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model serving by accessing seldon api server. Because Seldon API server provides an oauth, we need to receive a temporrary bearer token. We can receive this token by providing oauth key and secret that has been used in our `SeldonDeployment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_payload = {\n",
    "    \"data\":{\"ndarray\": [[\"try to stop flask from using multiple threads\"]]},\n",
    "}\n",
    "                         \n",
    "t = seldon.get_token(\n",
    "    server=SELDON_APISERVER_ADDR,\n",
    "    oauth_key=SELDON_OAUTH_KEY,\n",
    "    oauth_secret=SELDON_OAUTH_SECRET,\n",
    ")\n",
    "result = seldon.prediction(\n",
    "    server=SELDON_APISERVER_ADDR,\n",
    "    payload=test_payload,\n",
    "    token=t,\n",
    ") \n",
    "\n",
    "pd.DataFrame(data=result['data']['ndarray'], columns=['Predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a client application\n",
    "\n",
    "This section will be focused on application deployment routines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLICATION_NAME=f\"webapp-github\"\n",
    "APPLICATION_DOCKER_IMAGE = f\"{DOCKER_REGISTRY}/library/app:2\"\n",
    "APPLICATION_REPLICAS = 1\n",
    "SAMPLE_DATA='/data/sample.csv'\n",
    "GITHUB_TOKEN=get_secret('GITHUB_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User application has been implemented inside [src/app.py](src/app.py). We bake this applicaiton inside of docker container and deploy it further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template Dockerfile.flask -v\n",
    "ARG buildfrom=python:3.6\n",
    "FROM ${buildfrom}\n",
    "COPY src/flask-requirements.txt /tmp/requirements.txt\n",
    "RUN pip3 uninstall --yes numpy \\\n",
    "    && pip3 install --upgrade --no-cache-dir pip\n",
    "RUN pip3 install -U -r /tmp/requirements.txt \\\n",
    "    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* \\\n",
    "       /usr/share/man /usr/share/doc /usr/share/doc-base\n",
    "\n",
    "ADD {{SAMPLE_DATA_SET}} /data/sample.csv\n",
    "WORKDIR /src\n",
    "COPY src/app.py /src\n",
    "COPY src/templates /src/templates\n",
    "\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws.upload_to_s3(\n",
    "    destination=f\"s3://{S3_BUCKET}/{BUILD_CONTEXT}\",\n",
    "    ignorefile='kanikoignore.txt',\n",
    "    workspace='.',\n",
    "    s3_client=s3,\n",
    ")\n",
    "\n",
    "run = client.run_pipeline(exp.id, 'Build a serving image', 'kaniko.tar.gz', \n",
    "                          params={\n",
    "                              'image': APPLICATION_DOCKER_IMAGE,\n",
    "                              'build-context': f\"{MOUNT_PATH}/{BUILD_CONTEXT}\",\n",
    "                              'dockerfile': 'Dockerfile.flask',\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# block until job completion\n",
    "print(f\"Waiting for run: {run.id}...\")\n",
    "result = client.wait_for_run_completion(run.id, timeout=720).run.status\n",
    "print(f\"Finished with: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %templatefile templates/application.yaml -o application.yaml -v\n",
    "!kubectl apply -f application.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tear down\n",
    "\n",
    "Uppon completion, let's tear everything down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f bucket-volume.yaml\n",
    "!kubectl delete -f seldon.yaml\n",
    "!kubectl delete -f application.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
